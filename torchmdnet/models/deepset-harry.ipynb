{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-24T16:35:51.027701Z",
     "start_time": "2025-03-24T16:35:51.013010Z"
    }
   },
   "source": [
    "from typing import Callable, Dict, Optional, Union, List\n",
    "import typing\n",
    "import rich\n",
    "from rich import pretty\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric as tg\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.init import xavier_uniform_, zeros_\n",
    "from torch.nn import init, Dropout, BatchNorm1d\n",
    "\n",
    "from torch_geometric.nn import SAGEConv, GATConv\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "import torch_geometric.utils as pyg_utils\n",
    "from torch_geometric import edge_index\n",
    "\n",
    "from torch_geometric.nn import global_add_pool, global_max_pool, global_mean_pool\n",
    "\n",
    "from torchmdnet.models.utils import (\n",
    "\tNeighborEmbedding,\n",
    "\tCosineCutoff,\n",
    "\tOptimizedDistance,\n",
    "\trbf_class_mapping,\n",
    "\tact_class_mapping,\n",
    "\tscatter,\n",
    ")\n",
    "\n",
    "__all__ = [\"DeepSet\"]\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T16:35:57.155756Z",
     "start_time": "2025-03-24T16:35:57.136958Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DeepSet(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            base_cutoff: float,\n",
    "            outer_cutoff: float,\n",
    "            num_gates: int = 10,\n",
    "            k: int = 2,\n",
    "            max_num_neighbors: int = 400,\n",
    "            embedding_size: int = 256,\n",
    "            num_rbf=50,\n",
    "            expert_out_features: int = 128,\n",
    "            rbf_type: str = \"gauss\",\n",
    "            trainable_rbf: bool = False,\n",
    "            dtype: torch.dtype = torch.float32,\n",
    "            skip_duplicates: bool = False,\n",
    "    ):\n",
    "        super(DeepSet, self).__init__()\n",
    "\n",
    "        self.outer_cutoff = outer_cutoff\n",
    "        self.base_cutoff = base_cutoff\n",
    "        self.embedding_size = embedding_size\n",
    "        self.dtype = dtype\n",
    "        self.skip_duplicates = skip_duplicates\n",
    "        self.num_gates = num_gates\n",
    "\n",
    "        self.embedding = torch.nn.Linear(in_channels, hidden_channels)\n",
    "\n",
    "        self.sage1 = SAGEConv(2 * embedding_size, hidden_channels, aggr='mean')\n",
    "        self.sage2 = SAGEConv(hidden_channels, hidden_channels, aggr='sum')\n",
    "        self.sage3 = SAGEConv(hidden_channels, out_channels, aggr='max')\n",
    "\n",
    "        self.distance = OptimizedDistance(\n",
    "            base_cutoff,\n",
    "            outer_cutoff,\n",
    "            max_num_pairs=max_num_neighbors,\n",
    "            return_vecs=True,\n",
    "            loop=True,\n",
    "            box=None,\n",
    "            long_edge_index=True,\n",
    "            check_errors=True,\n",
    "        )\n",
    "\n",
    "        self.distance_expansion = rbf_class_mapping[rbf_type](\n",
    "            base_cutoff, outer_cutoff, num_rbf, trainable_rbf\n",
    "        )\n",
    "        self.distance_proj = nn.Linear(num_rbf, embedding_size, dtype=dtype)\n",
    "        self.cutoff = CosineCutoff(base_cutoff, outer_cutoff)\n",
    "        self.embedding = nn.Embedding(100, embedding_size, dtype=dtype)\n",
    "\n",
    "        self.neighbor_embedding = NeighborEmbedding(embedding_size, 20, base_cutoff, outer_cutoff, 100, dtype)\n",
    "        expanded_feature_dim = embedding_size + 3\n",
    "        self.d_ij_transform = nn.Linear(expanded_feature_dim, embedding_size, dtype=dtype)\n",
    "        self.a_i_transform = nn.Linear(embedding_size, embedding_size, dtype=dtype)\n",
    "        self.a_j_transform = nn.Linear(embedding_size, embedding_size, dtype=dtype)\n",
    "\n",
    "        self.gamma_transform = nn.Linear(3 * embedding_size, embedding_size, dtype=dtype)\n",
    "        self.W_g = nn.Parameter(torch.randn(embedding_size, num_gates) * 0.01)\n",
    "        self.W_noise = nn.Parameter(torch.randn(embedding_size, num_gates) * 0.01)\n",
    "        self.W_distance = nn.Parameter(torch.randn(embedding_size, num_gates) * 0.01)\n",
    "\n",
    "        self.t_parameters = nn.Parameter(torch.randn(num_gates) * 0.01)\n",
    "\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Linear(embedding_size, expert_out_features, dtype=dtype)\n",
    "            for _ in range(num_gates)\n",
    "        ])\n",
    "        self.batch_norm1 = BatchNorm1d(hidden_channels)\n",
    "        self.batch_norm2 = BatchNorm1d(hidden_channels)\n",
    "        self.dropout = Dropout(p=0.1)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self,\n",
    "                z: torch.Tensor,\n",
    "                pos: torch.Tensor,\n",
    "                batch: torch.Tensor,\n",
    "                box: Optional[torch.Tensor] = None,\n",
    "                q: Optional[torch.Tensor] = None,\n",
    "                s: Optional[torch.Tensor] = None) -> typing.Tuple[\n",
    "            torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        x = self.embedding(z)\n",
    "\n",
    "        edge_index, edge_weight, edge_vec = self.distance(pos, batch, box)\n",
    "\n",
    "        edge_attr = self.distance_expansion(edge_weight)\n",
    "\n",
    "        mask = edge_index[0] != edge_index[1]\n",
    "        if not mask.all():\n",
    "            edge_index = edge_index[:, mask]\n",
    "            edge_weight = edge_weight[mask]\n",
    "            edge_attr = edge_attr[mask]\n",
    "            edge_vec = edge_vec[mask]\n",
    "\n",
    "        if self.skip_duplicates:\n",
    "            edge_index = edge_index[:, ::2]\n",
    "            edge_weight = edge_weight[::2]\n",
    "            edge_attr = edge_attr[::2]\n",
    "            edge_vec = edge_vec[::2]\n",
    "\n",
    "        edge_weight_cube = edge_weight ** 3\n",
    "        edge_weight_sqrt = torch.sqrt(edge_weight)\n",
    "\n",
    "        edge_vec = edge_vec / torch.norm(edge_vec, dim=1, keepdim=True)\n",
    "\n",
    "        C = self.cutoff(edge_weight)\n",
    "        d_ij_projection = self.distance_proj(edge_attr) * C.view(-1, 1)\n",
    "\n",
    "        edge_features = torch.cat(\n",
    "            [\n",
    "                d_ij_projection,\n",
    "                edge_weight_cube.view(-1, 1),\n",
    "                edge_weight_sqrt.view(-1, 1),\n",
    "                edge_weight.view(-1, 1)\n",
    "            ], dim=1\n",
    "        )\n",
    "\n",
    "        d_ij_t_projection = self.d_ij_transform(edge_features)\n",
    "        a_i_projection = self.a_i_transform(x[edge_index[0, :]])\n",
    "        a_j_projection = self.a_j_transform(x[edge_index[1, :]])\n",
    "\n",
    "        gamma_projection = self.gamma_transform(torch.concat(\n",
    "            [a_i_projection.squeeze(), a_j_projection.squeeze(), d_ij_t_projection], dim=1))\n",
    "\n",
    "        d_ij_expanded = torch.repeat_interleave(edge_weight, self.num_gates, dim=0)\n",
    "        d_ij_expanded = 1 / torch.clamp(\n",
    "            torch.abs(d_ij_expanded.view(\n",
    "                -1,\n",
    "                self.t_parameters.size(0)\n",
    "            ) - self.t_parameters),\n",
    "            min=1e-8\n",
    "        )\n",
    "        softmax_d_ij_expanded = F.softmax(d_ij_expanded, dim=1)\n",
    "\n",
    "        experts_output = [self.experts[i](gamma_projection) for i in range(self.num_gates)]\n",
    "\n",
    "        experts_contributions = [e_o * e_w for e_o, e_w in zip(experts_output, softmax_d_ij_expanded.split(1, dim=1))]\n",
    "        edge_level_output = torch.sum(torch.stack(experts_contributions, dim=0), dim=0)\n",
    "\n",
    "        atom_level_output_x = scatter(edge_level_output, edge_index[0], dim=0, reduce=\"sum\")\n",
    "\n",
    "        n_atoms = z.size(0)\n",
    "        atom_level_output = torch.zeros(n_atoms, self.experts[0].out_features, dtype=self.dtype, device=z.device)\n",
    "        vec = torch.zeros(n_atoms, 3, self.experts[0].out_features, dtype=self.dtype, device=z.device)\n",
    "\n",
    "        atom_level_output.index_add_(0, edge_index[0], edge_level_output)\n",
    "\n",
    "        edge_vec_expanded = edge_vec.unsqueeze(-1).repeat(1, 1, self.experts[\n",
    "            0].out_features)\n",
    "        weighted_edge_vec = edge_vec_expanded * edge_level_output.unsqueeze(1)\n",
    "        for dim in range(3):\n",
    "            weighted_edge_vec_dim = weighted_edge_vec[:, dim, :]\n",
    "            vec[:, dim, :].index_add_(0, edge_index[0], weighted_edge_vec_dim)\n",
    "\n",
    "        # @todo New code here GNN testng SAGE, concat edge features to pass all structural info into the model\n",
    "        aggregated_edge_features = scatter(edge_features, edge_index[0], dim=0, dim_size=x.size(0),reduce=\"mean\")\n",
    "        x = torch.cat([x, aggregated_edge_features], dim=1)\n",
    "\n",
    "        x = self.sage1(atom_level_output_x, edge_index)\n",
    "        x = F.silu(self.batch_norm1(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.sage2(x, edge_index)\n",
    "        x = F.silu(self.batch_norm2(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.sage3(x, edge_index)\n",
    "\n",
    "        # @todo pooling may or may not be useful i will run with and without.\n",
    "        out = global_mean_pool(x, batch)\n",
    "\n",
    "        return x, vec, z, pos, batch, out"
   ],
   "id": "7d4ee25a5eef899e",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T12:23:31.169041Z",
     "start_time": "2025-03-19T12:23:31.166626Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\tdef reset_parameters(self):\n",
    "\t\t... #Ngl i do not know what this does"
   ],
   "id": "74ce92ebcf36c1cb",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T12:23:31.490580Z",
     "start_time": "2025-03-19T12:23:31.482319Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 57,
   "source": [
    "def forward(self,\n",
    "            z: torch.Tensor,  # Type hints, here for readability and clarity\n",
    "            pos: torch.Tensor,\n",
    "            batch: torch.Tensor,\n",
    "            edge_index: torch.Tensor,\n",
    "            box: Optional[torch.Tensor] = None,\n",
    "            edge_attr: torch.Tensor = None,\n",
    "            q: Optional[torch.Tensor] = None,\n",
    "            s: Optional[torch.Tensor] = None) -> typing.Tuple[\n",
    "    torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:  # Model returns 5 tensors as input does not = output :)\n",
    "    \"\"\"\n",
    "    # Google style doc string\n",
    "    Args:\n",
    "        z:                                  # Size is like (n_atoms, 1)\n",
    "        pos:                                # Size is like (n_atoms, 3)\n",
    "        batch:                              # Size is like (n_atoms, 1)\n",
    "        box:                                # Size is like (3, 3)\n",
    "        q:                                  # Size is like (n_atoms, 1)\n",
    "        s:                                  # Size is like (n_atoms, 1)\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    x = self.node_embedding(z)\n",
    "    # Beginning the forward pass\n",
    "    # x = self.embedding(z) # Maps atomic numbers to learnable vectors, continuous representation\n",
    "\n",
    "    edge_index, edge_weight, edge_vec = self.distance(pos, batch, box)  # Finds pairs of atoms close and returns which atoms are connected, their distance and direction\n",
    "    if edge_attr is not None:\n",
    "        edge_attr = self.edge_embedding(edge_attr)\n",
    "\n",
    "    edge_weight: torch.Tensor  # Type hint, edge_weight is expected to be a tensor\n",
    "\n",
    "    # print(x_0)\n",
    "    edge_attr = self.distance_expansion(edge_weight)  # Converts raw distances between atoms into a better format, NN struggle with raw values\n",
    "\n",
    "    # edge_index = edge_index[:, edge_weight != 0]\n",
    "    # edge_weight = edge_weight[edge_weight != 0] # * Prevent self loops (No distance between same an atom so it should be 0)\n",
    "    # edge_vec = edge_vec[edge_vec != 0]\n",
    "    mask = edge_index[0] != edge_index[1]  # Mask used to handle anomalous data points, filters out self loops (source and target)\n",
    "    if not mask.all():  # Checks for any false values in the mask, if so then following code is executed\n",
    "        edge_index = edge_index[:, mask]\n",
    "        edge_weight = edge_weight[mask]\n",
    "        edge_attr = edge_attr[mask]\n",
    "        edge_vec = edge_vec[mask]  # Mask edge_vec as well\n",
    "\n",
    "    if self.skip_duplicates:  # this removes repeated edges in calculation (it means upper triangle matrix)\n",
    "        edge_index = edge_index[:, ::2]  # Slicing, selects every second edge so skips over dupes\n",
    "        edge_weight = edge_weight[::2]  # Slice with 2 to skip duplicate edges and keep only one direction\n",
    "        edge_attr = edge_attr[::2]\n",
    "        edge_vec = edge_vec[::2]\n",
    "\n",
    "    # Normalize edge_vec for masked edges (similar to TorchMD_ET)\n",
    "    edge_vec = edge_vec / torch.norm(edge_vec, dim=1, keepdim=True)\n",
    "\n",
    "    # Compute the cutoff and distance projection\n",
    "    C = self.cutoff(edge_weight)  # Applies cutoff function to edge weight tensor, limits interactions to certain threshold.\n",
    "    d_ij_projection = self.distance_proj(edge_attr) * C.view(-1, 1)  # Applies cutoff values to projections. If edge is zero then it is reflected here.\n",
    "\n",
    "    # Transform the distance projection, nuclear charges and atom embeddings\n",
    "    d_ij_t_projection = self.d_ij_transform(d_ij_projection)\n",
    "    a_i_projection = self.a_i_transform(x[edge_index[0, :]])  # Prepares for calculations\n",
    "    a_j_projection = self.a_j_transform(x[edge_index[1, :]])  # Enables message passing\n",
    "\n",
    "    gamma_projection = self.gamma_transform(torch.concat([a_i_projection.squeeze(), a_j_projection.squeeze(), d_ij_t_projection], dim=1))\n",
    "    # Combine the source and target atom features then apply transformation to better learn them\n",
    "\n",
    "    # Computing Z:\n",
    "    d_ij_expanded = torch.repeat_interleave(edge_weight, self.num_gates, dim=0)\n",
    "\n",
    "    d_ij_expanded = 1 / torch.clamp(\n",
    "        torch.abs(d_ij_expanded.view(\n",
    "            -1,\n",
    "            self.t_parameters.size(0)\n",
    "        ) - self.t_parameters),\n",
    "        min=1e-8  # Avoid division by zero\n",
    "    )\n",
    "    softmax_d_ij_expanded = F.softmax(d_ij_expanded, dim=1)\n",
    "\n",
    "    experts_output = [self.experts[i](gamma_projection) for i in range(self.num_gates)]\n",
    "\n",
    "    experts_contributions = [e_o * e_w for e_o, e_w in zip(experts_output, softmax_d_ij_expanded.split(1, dim=1))]\n",
    "    # do the sum of the experts_contributions\n",
    "    edge_level_output = torch.sum(torch.stack(experts_contributions, dim=0), dim=0)\n",
    "\n",
    "    # Doing aggregation over the atoms\n",
    "    atom_level_output_x = scatter(edge_level_output, edge_index[0], dim=0, reduce=\"sum\")\n",
    "\n",
    "    # Doing the equivariant operation\n",
    "    n_atoms = z.size(0)  # Number of atoms from z\n",
    "    atom_level_output = torch.zeros(n_atoms, self.experts[0].out_features, dtype=self.dtype, device=z.device)\n",
    "    vec = torch.zeros(n_atoms, 3, self.experts[0].out_features, dtype=self.dtype,\n",
    "                      device=z.device)  # Atom-level vector features\n",
    "\n",
    "    # Aggregate edge-level scalar output to atom-level using scatter_reduce (sum)\n",
    "    atom_level_output.index_add_(0, edge_index[0], edge_level_output)  # Sum edge outputs to source atoms\n",
    "\n",
    "    # Aggregate edge-level vector features to atom-level using scatter_reduce (sum)\n",
    "    # Map edge_vec (shape: (num_edges, 3)) to atom-level vec\n",
    "    # First, expand edge_vec to match expert_out_features\n",
    "    edge_vec_expanded = edge_vec.unsqueeze(-1).repeat(1, 1, self.experts[\n",
    "        0].out_features)  # Shape: (num_edges, 3, expert_out_features)\n",
    "    # Weight edge_vec_expanded by edge_level_output (broadcasting)\n",
    "    weighted_edge_vec = edge_vec_expanded * edge_level_output.unsqueeze(\n",
    "        1)  # Shape: (num_edges, 3, expert_out_features)\n",
    "    # Aggregate to atom-level using scatter_reduce (sum) for source atoms\n",
    "    for dim in range(3):  # Iterate over spatial dimensions (0, 1, 2)\n",
    "        # Extract the dim-th spatial component of weighted_edge_vec\n",
    "        weighted_edge_vec_dim = weighted_edge_vec[:, dim, :]  # Shape: (num_edges, expert_out_features)\n",
    "        # Aggregate to atom-level using index_add_ for source atoms\n",
    "        vec[:, dim, :].index_add_(0, edge_index[0], weighted_edge_vec_dim)\n",
    "\n",
    "    return atom_level_output_x, vec, z, pos, batch"
   ],
   "id": "ae69588b39844e16"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T12:23:31.914064Z",
     "start_time": "2025-03-19T12:23:31.910201Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 58,
   "source": [
    "#\n",
    "\t\t# # Apply Noisy Top-K Gating to d_ij_t_projection\n",
    "\t\t# # Step 1: Compute H(x) with noise (Equation 4)\n",
    "\t\t# gating_scores = torch.matmul(d_ij_t_projection, self.W_g)  # x · W_g\n",
    "\t\t# noise_component = torch.randn_like(gating_scores) * F.softplus(\n",
    "\t\t# \ttorch.matmul(d_ij_t_projection, self.W_noise))  # StandardNormal() · Softplus(x · W_noise)\n",
    "\t\t# H_x = gating_scores + noise_component  # H(x)_i\n",
    "\t\t#\n",
    "\t\t# # Step 2: Apply KeepTopK (Equation 5)\n",
    "\t\t# # Keep only the top k values, set others to -inf\n",
    "\t\t# top_k_values, top_k_indices = torch.topk(H_x, self.k, dim=-1)  # Get top k values\n",
    "\t\t# mask_top_k = H_x >= top_k_values[..., -1:]  # Create mask for top k\n",
    "\t\t# H_x[~mask_top_k] = float('-inf')  # Set non-top-k values to -inf\n",
    "\t\t#\n",
    "\t\t# # Step 3: Apply Softmax (Equation 3)\n",
    "\t\t# gating_output = F.softmax(H_x, dim=-1)  # G(x) = Softmax(KeepTopK(H(x), k))\n",
    "\t\t#\n",
    "\t\t# # Replace softmax_gamma_projection with gating_output\n",
    "\t\t# # softmax_gamma_projection = gating_output\n",
    "\t\t#\n",
    "\t\t# # Vectorized expert routing using gamma_projection\n",
    "\t\t# # Ensure num_gates >= k to avoid out-of-bounds indices\n",
    "\t\t# if self.k > self.num_gates:\n",
    "\t\t# \traise ValueError(f\"k ({self.k}) cannot be greater than num_gates ({self.num_gates})\")\n",
    "\t\t#\n",
    "\t\t# # Expand gamma_projection to include k dimension (for top-k routing)\n",
    "\t\t# gamma_expanded = gamma_projection.unsqueeze(1).repeat(1, self.k, 1)  # Shape: (num_edges, k, embedding_size)\n",
    "\t\t#\n",
    "\t\t# # Adjust top_k_indices to be within bounds (0 to k-1) if needed, but ensure they map to valid gates\n",
    "\t\t# # Since top_k_indices comes from H_x (dim=num_gates), clip or validate indices\n",
    "\t\t# top_k_indices = top_k_indices.clamp(0, self.num_gates - 1)  # Ensure indices are within [0, num_gates-1]\n",
    "\t\t#\n",
    "\t\t# # Create expert indices for routing (map top_k_indices to expert indices, 0 to k-1 for simplicity)\n",
    "\t\t# expert_indices = top_k_indices % self.k  # Map to 0 to k-1 for expert selection (simplified routing)\n",
    "\t\t#\n",
    "\t\t# # Gather gamma_projection for each expert (broadcasting top-k indices)\n",
    "\t\t# # Use expert_indices to route to the k experts\n",
    "\t\t# expert_inputs = gamma_expanded.gather(1, expert_indices.unsqueeze(-1).expand(-1, -1, gamma_projection.size(\n",
    "\t\t# \t-1)))  # Shape: (num_edges, k, embedding_size)\n",
    "\t\t#\n",
    "\t\t# # Process through experts (vectorized)\n",
    "\t\t# # Stack expert outputs for all k experts\n",
    "\t\t# expert_outputs = torch.stack([self.experts[i](expert_inputs[:, i]) for i in range(self.k)], dim=1)\n",
    "\t\t# # Shape: (num_edges, k, expert_out_features)\n",
    "\t\t#\n",
    "\t\t# # Aggregate expert outputs using gating weights\n",
    "\t\t# gate_weights = gating_output.gather(1, top_k_indices)  # Shape: (num_edges, k)\n",
    "\t\t# gate_weights = gate_weights.unsqueeze(-1)  # Shape: (num_edges, k, 1)\n",
    "\t\t# edge_level_output = (expert_outputs * gate_weights).sum(dim=1)  # Shape: (num_edges, expert_out_features)\n",
    "\t\t#\n",
    "\t\t# # Aggregate edge-level output to atom-level using scatter_reduce\n",
    "\t\t# n_atoms = z.size(0)  # Number of atoms from z\n",
    "\t\t# atom_level_output = torch.zeros(n_atoms, self.experts[0].out_features, dtype=self.dtype, device=z.device)\n",
    "\t\t# vec = torch.zeros(n_atoms, 3, self.experts[0].out_features, dtype=self.dtype,\n",
    "\t\t#                   device=z.device)  # Atom-level vector features\n",
    "\t\t#\n",
    "\t\t# # Aggregate edge-level scalar output to atom-level using scatter_reduce (sum)\n",
    "\t\t# atom_level_output.index_add_(0, edge_index[0], edge_level_output)  # Sum edge outputs to source atoms\n",
    "\t\t#\n",
    "\t\t# # Aggregate edge-level vector features to atom-level using scatter_reduce (sum)\n",
    "\t\t# # Map edge_vec (shape: (num_edges, 3)) to atom-level vec\n",
    "\t\t# # First, expand edge_vec to match expert_out_features\n",
    "\t\t# edge_vec_expanded = edge_vec.unsqueeze(-1).repeat(1, 1, self.experts[\n",
    "\t\t# \t0].out_features)  # Shape: (num_edges, 3, expert_out_features)\n",
    "\t\t# # Weight edge_vec_expanded by edge_level_output (broadcasting)\n",
    "\t\t# weighted_edge_vec = edge_vec_expanded * edge_level_output.unsqueeze(\n",
    "\t\t# \t1)  # Shape: (num_edges, 3, expert_out_features)\n",
    "\t\t# # Aggregate to atom-level using scatter_reduce (sum) for source atoms\n",
    "\t\t# for dim in range(3):  # Iterate over spatial dimensions (0, 1, 2)\n",
    "\t\t# \t# Extract the dim-th spatial component of weighted_edge_vec\n",
    "\t\t# \tweighted_edge_vec_dim = weighted_edge_vec[:, dim, :]  # Shape: (num_edges, expert_out_features)\n",
    "\t\t# \t# Aggregate to atom-level using index_add_ for source atoms\n",
    "\t\t# \tvec[:, dim, :].index_add_(0, edge_index[0], weighted_edge_vec_dim)\n",
    "\t\t# # vec.index_add_(0, edge_index[0].unsqueeze(1).expand(-1, 3), weighted_edge_vec)\n",
    "\t\t#\n",
    "\t\t# # Optionally, normalize by the number of edges per atom if needed (e.g., for mean)\n",
    "\t\t# # edge_counts = torch.bincount(edge_index[0], minlength=n_atoms).float()\n",
    "\t\t# # edge_counts = edge_counts.clamp(min=1)  # Avoid division by zero\n",
    "\t\t# # atom_level_output = atom_level_output / edge_counts.unsqueeze(-1)\n",
    "\t\t# # vec = vec / edge_counts.unsqueeze(1).unsqueeze(-1)  # Normalize vec similarly\n",
    "\t\t#\n",
    "\t\t# # Replace softmax_gamma_projection with gating_output or atom-level output, depending on return\n",
    "\t\t# softmax_gamma_projection = gating_output  # Keep gating output for consistency (not used in return here)\n",
    "\t\t#\n",
    "\t\t# # Return tuple matching TorchMD_ET structure\n",
    "\t\t# return atom_level_output, vec, z, pos, batch\n",
    "\t\t#"
   ],
   "id": "51e9c1abd1bb4444"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T14:41:27.933386Z",
     "start_time": "2025-03-19T14:41:27.924054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "\timport torch\n",
    "\timport numpy as np\n",
    "\timport random\n",
    "\timport time\n",
    "\n",
    "\n",
    "\tdef set_seed(seed: int):\n",
    "\t\ttorch.manual_seed(seed)\n",
    "\t\tnp.random.seed(seed)\n",
    "\t\trandom.seed(seed)\n",
    "\t\tif torch.cuda.is_available():\n",
    "\t\t\ttorch.cuda.manual_seed(seed)\n",
    "\t\t\ttorch.cuda.manual_seed_all(seed)\n",
    "\t\ttorch.backends.cudnn.deterministic = True\n",
    "\t\ttorch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\t# Define the forward method in DeepSet\n",
    "\tclass DeepSet(nn.Module):\n",
    "\t\tdef __init__(self, num_node_features, gcn_hidden_dim, num_atom_features, base_cutoff, outer_cutoff):\n",
    "\t\t\tsuper().__init__()\n",
    "\t\t\t# Add any necessary layer initializations here\n",
    "\t\t\tself.num_node_features = num_node_features\n",
    "\t\t\tself.gcn_hidden_dim = gcn_hidden_dim\n",
    "\t\t\tself.num_atom_features = num_atom_features\n",
    "\t\t\tself.base_cutoff = base_cutoff\n",
    "\t\t\tself.outer_cutoff = outer_cutoff\n",
    "\n",
    "\t\t\t# Example embeddings and layers:\n",
    "\t\t\tself.node_embedding = nn.Embedding(num_node_features, gcn_hidden_dim)\n",
    "\t\t\tself.edge_embedding = nn.Linear(3, gcn_hidden_dim)  # Corrected input to match the shape of `pos`\n",
    "\t\t\tself.experts = nn.ModuleList(\n",
    "\t\t\t\t[nn.Linear(gcn_hidden_dim, gcn_hidden_dim) for _ in range(10)]  # Assume 10 experts\n",
    "\t\t\t)\n",
    "\n",
    "\t\tdef forward(self, z, pos, batch):\n",
    "\t\t\t# Example forward pass using dummy functionality:\n",
    "\t\t\tz_embedding = self.node_embedding(z.squeeze(-1))\n",
    "\t\t\tpos_embedding = self.edge_embedding(pos)\n",
    "\n",
    "\t\t\t# Example operation with experts\n",
    "\t\t\toutputs = [expert(z_embedding) for expert in self.experts]\n",
    "\t\t\toutput = sum(outputs)  # Sum all expert outputs (replace with implementation logic)\n",
    "\n",
    "\t\t\t# Return example tensors\n",
    "\t\t\treturn output, pos_embedding, z, pos, batch\n",
    "\n",
    "\n",
    "\t# Set the seed\n",
    "\tset_seed(2000)\n",
    "\n",
    "\t# Initialize the model\n",
    "\tmodel = DeepSet(\n",
    "                   num_node_features=100,\n",
    "                   gcn_hidden_dim=64,\n",
    "                   num_atom_features=10,  # Added the missing argument\n",
    "                   base_cutoff=0.0,\n",
    "                   outer_cutoff=3.0,\n",
    "\t)\n",
    "\n",
    "\t# Generate random input\n",
    "\tz = torch.randint(0, 100, (18, 1))\n",
    "\tpos = torch.randint(0, 5, (18, 3), dtype=torch.float32)\n",
    "\tbatch = torch.zeros(18, dtype=torch.long)\n",
    "\n",
    "\t# Test the forward method of the model\n",
    "\tx, vec, z, pos, batch = model(z, pos, batch)"
   ],
   "id": "b80ce4b9264d53d5",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8a9a28c010ef69c1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
